{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nathan/.conda/envs/torchmocks/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torchvision.models import resnet152, feature_extraction\n",
    "import torch\n",
    "from torchmocks import mock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations_wishlist = [\n",
    "    value\n",
    "    for key, value in torch.nn.modules.activation.__dict__.items()\n",
    "    if isinstance(value, type) and issubclass(value, torch.nn.Module)\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<class 'torch.nn.modules.linear.NonDynamicallyQuantizableLinear'>\n",
      "<class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "<class 'torch.nn.modules.dropout.Dropout'>\n"
     ]
    }
   ],
   "source": [
    "net = torch.nn.Transformer(nhead=16, num_encoder_layers=12)\n",
    "src = torch.rand((10, 32, 512))\n",
    "tgt = torch.rand((20, 32, 512))\n",
    "mock(net, debug=True)\n",
    "out = net.encoder(src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.nn.modules.linear.Linear"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.encoder.layers[0].linear1.__class__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "       grad_fn=<MockLinearFunctionBackward>)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.encoder.layers[0].linear1(src )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerEncoder(\n",
       "  (layers): ModuleList(\n",
       "    (0): TransformerEncoderLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (dropout2): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerEncoderLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (dropout2): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerEncoderLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (dropout2): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerEncoderLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (dropout2): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerEncoderLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (dropout2): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerEncoderLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (dropout2): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerEncoderLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (dropout2): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerEncoderLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (dropout2): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerEncoderLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (dropout2): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerEncoderLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (dropout2): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerEncoderLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (dropout2): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerEncoderLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (dropout2): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.encoder:w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndql = torch.nn.modules.linear.NonDynamicallyQuantizableLinear(3, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'training': True,\n",
       " '_parameters': OrderedDict([('weight', Parameter containing:\n",
       "               tensor([[ 0.0549,  0.5747,  0.4613],\n",
       "                       [-0.4003, -0.5445,  0.0228],\n",
       "                       [-0.3018, -0.2229,  0.4856],\n",
       "                       [ 0.3716,  0.5621,  0.1002],\n",
       "                       [-0.4930,  0.2379,  0.5583],\n",
       "                       [-0.3755, -0.4162, -0.2658],\n",
       "                       [-0.4989, -0.4700, -0.0420],\n",
       "                       [ 0.0193,  0.2194,  0.0210]], requires_grad=True)),\n",
       "              ('bias',\n",
       "               Parameter containing:\n",
       "               tensor([-0.3542,  0.2347, -0.1731,  0.2280,  0.3959, -0.4210,  0.2893, -0.1959],\n",
       "                      requires_grad=True))]),\n",
       " '_buffers': OrderedDict(),\n",
       " '_non_persistent_buffers_set': set(),\n",
       " '_backward_hooks': OrderedDict(),\n",
       " '_is_full_backward_hook': None,\n",
       " '_forward_hooks': OrderedDict(),\n",
       " '_forward_pre_hooks': OrderedDict(),\n",
       " '_state_dict_hooks': OrderedDict(),\n",
       " '_load_state_dict_pre_hooks': OrderedDict(),\n",
       " '_load_state_dict_post_hooks': OrderedDict(),\n",
       " '_modules': OrderedDict(),\n",
       " 'in_features': 3,\n",
       " 'out_features': 8}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ndql.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 32, 512])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.nn.Transformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m\n",
      "\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0msrc\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtgt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0msrc_mask\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtgt_mask\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmemory_mask\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0msrc_key_padding_mask\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtgt_key_padding_mask\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmemory_key_padding_mask\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "Take in and process masked source/target sequences.\n",
      "\n",
      "Args:\n",
      "    src: the sequence to the encoder (required).\n",
      "    tgt: the sequence to the decoder (required).\n",
      "    src_mask: the additive mask for the src sequence (optional).\n",
      "    tgt_mask: the additive mask for the tgt sequence (optional).\n",
      "    memory_mask: the additive mask for the encoder output (optional).\n",
      "    src_key_padding_mask: the ByteTensor mask for src keys per batch (optional).\n",
      "    tgt_key_padding_mask: the ByteTensor mask for tgt keys per batch (optional).\n",
      "    memory_key_padding_mask: the ByteTensor mask for memory keys per batch (optional).\n",
      "\n",
      "Shape:\n",
      "    - src: :math:`(S, E)` for unbatched input, :math:`(S, N, E)` if `batch_first=False` or\n",
      "      `(N, S, E)` if `batch_first=True`.\n",
      "    - tgt: :math:`(T, E)` for unbatched input, :math:`(T, N, E)` if `batch_first=False` or\n",
      "      `(N, T, E)` if `batch_first=True`.\n",
      "    - src_mask: :math:`(S, S)` or :math:`(N\\cdot\\text{num\\_heads}, S, S)`.\n",
      "    - tgt_mask: :math:`(T, T)` or :math:`(N\\cdot\\text{num\\_heads}, T, T)`.\n",
      "    - memory_mask: :math:`(T, S)`.\n",
      "    - src_key_padding_mask: :math:`(S)` for unbatched input otherwise :math:`(N, S)`.\n",
      "    - tgt_key_padding_mask: :math:`(T)` for unbatched input otherwise :math:`(N, T)`.\n",
      "    - memory_key_padding_mask: :math:`(S)` for unbatched input otherwise :math:`(N, S)`.\n",
      "\n",
      "    Note: [src/tgt/memory]_mask ensures that position i is allowed to attend the unmasked\n",
      "    positions. If a ByteTensor is provided, the non-zero positions are not allowed to attend\n",
      "    while the zero positions will be unchanged. If a BoolTensor is provided, positions with ``True``\n",
      "    are not allowed to attend while ``False`` values will be unchanged. If a FloatTensor\n",
      "    is provided, it will be added to the attention weight.\n",
      "    [src/tgt/memory]_key_padding_mask provides specified elements in the key to be ignored by\n",
      "    the attention. If a ByteTensor is provided, the non-zero positions will be ignored while the zero\n",
      "    positions will be unchanged. If a BoolTensor is provided, the positions with the\n",
      "    value of ``True`` will be ignored while the position with the value of ``False`` will be unchanged.\n",
      "\n",
      "    - output: :math:`(T, E)` for unbatched input, :math:`(T, N, E)` if `batch_first=False` or\n",
      "      `(N, T, E)` if `batch_first=True`.\n",
      "\n",
      "    Note: Due to the multi-head attention architecture in the transformer model,\n",
      "    the output sequence length of a transformer is same as the input sequence\n",
      "    (i.e. target) length of the decode.\n",
      "\n",
      "    where S is the source sequence length, T is the target sequence length, N is the\n",
      "    batch size, E is the feature number\n",
      "\n",
      "Examples:\n",
      "    >>> output = transformer_model(src, tgt, src_mask=src_mask, tgt_mask=tgt_mask)\n",
      "\u001b[0;31mFile:\u001b[0m      ~/.local/lib/python3.9/site-packages/torch/nn/modules/transformer.py\n",
      "\u001b[0;31mType:\u001b[0m      method\n"
     ]
    }
   ],
   "source": [
    "t.forward?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (2): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (3): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (4): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (5): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder): TransformerDecoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerDecoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (multihead_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): TransformerDecoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (multihead_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (2): TransformerDecoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (multihead_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (3): TransformerDecoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (multihead_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (4): TransformerDecoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (multihead_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (5): TransformerDecoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (multihead_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (2): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (3): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (4): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (5): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder): TransformerDecoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerDecoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (multihead_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): TransformerDecoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (multihead_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (2): TransformerDecoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (multihead_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (3): TransformerDecoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (multihead_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (4): TransformerDecoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (multihead_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (5): TransformerDecoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (multihead_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.zeros((4, 32, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m\n",
      "\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0msrc\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtgt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0msrc_mask\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtgt_mask\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmemory_mask\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0msrc_key_padding_mask\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtgt_key_padding_mask\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmemory_key_padding_mask\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "Take in and process masked source/target sequences.\n",
      "\n",
      "Args:\n",
      "    src: the sequence to the encoder (required).\n",
      "    tgt: the sequence to the decoder (required).\n",
      "    src_mask: the additive mask for the src sequence (optional).\n",
      "    tgt_mask: the additive mask for the tgt sequence (optional).\n",
      "    memory_mask: the additive mask for the encoder output (optional).\n",
      "    src_key_padding_mask: the ByteTensor mask for src keys per batch (optional).\n",
      "    tgt_key_padding_mask: the ByteTensor mask for tgt keys per batch (optional).\n",
      "    memory_key_padding_mask: the ByteTensor mask for memory keys per batch (optional).\n",
      "\n",
      "Shape:\n",
      "    - src: :math:`(S, E)` for unbatched input, :math:`(S, N, E)` if `batch_first=False` or\n",
      "      `(N, S, E)` if `batch_first=True`.\n",
      "    - tgt: :math:`(T, E)` for unbatched input, :math:`(T, N, E)` if `batch_first=False` or\n",
      "      `(N, T, E)` if `batch_first=True`.\n",
      "    - src_mask: :math:`(S, S)` or :math:`(N\\cdot\\text{num\\_heads}, S, S)`.\n",
      "    - tgt_mask: :math:`(T, T)` or :math:`(N\\cdot\\text{num\\_heads}, T, T)`.\n",
      "    - memory_mask: :math:`(T, S)`.\n",
      "    - src_key_padding_mask: :math:`(S)` for unbatched input otherwise :math:`(N, S)`.\n",
      "    - tgt_key_padding_mask: :math:`(T)` for unbatched input otherwise :math:`(N, T)`.\n",
      "    - memory_key_padding_mask: :math:`(S)` for unbatched input otherwise :math:`(N, S)`.\n",
      "\n",
      "    Note: [src/tgt/memory]_mask ensures that position i is allowed to attend the unmasked\n",
      "    positions. If a ByteTensor is provided, the non-zero positions are not allowed to attend\n",
      "    while the zero positions will be unchanged. If a BoolTensor is provided, positions with ``True``\n",
      "    are not allowed to attend while ``False`` values will be unchanged. If a FloatTensor\n",
      "    is provided, it will be added to the attention weight.\n",
      "    [src/tgt/memory]_key_padding_mask provides specified elements in the key to be ignored by\n",
      "    the attention. If a ByteTensor is provided, the non-zero positions will be ignored while the zero\n",
      "    positions will be unchanged. If a BoolTensor is provided, the positions with the\n",
      "    value of ``True`` will be ignored while the position with the value of ``False`` will be unchanged.\n",
      "\n",
      "    - output: :math:`(T, E)` for unbatched input, :math:`(T, N, E)` if `batch_first=False` or\n",
      "      `(N, T, E)` if `batch_first=True`.\n",
      "\n",
      "    Note: Due to the multi-head attention architecture in the transformer model,\n",
      "    the output sequence length of a transformer is same as the input sequence\n",
      "    (i.e. target) length of the decode.\n",
      "\n",
      "    where S is the source sequence length, T is the target sequence length, N is the\n",
      "    batch size, E is the feature number\n",
      "\n",
      "Examples:\n",
      "    >>> output = transformer_model(src, tgt, src_mask=src_mask, tgt_mask=tgt_mask)\n",
      "\u001b[0;31mFile:\u001b[0m      ~/.local/lib/python3.9/site-packages/torch/nn/modules/transformer.py\n",
      "\u001b[0;31mType:\u001b[0m      method\n"
     ]
    }
   ],
   "source": [
    "t.forward?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerEncoder(\n",
       "  (layers): ModuleList(\n",
       "    (0): TransformerEncoderLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (dropout2): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerEncoderLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (dropout2): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerEncoderLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (dropout2): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerEncoderLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (dropout2): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerEncoderLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (dropout2): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerEncoderLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (dropout2): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t._modules['encoder']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "wishlist = [\n",
    "    value\n",
    "    for key, value in torch.nn.modules.__dict__.items()\n",
    "    if isinstance(value, type) and issubclass(value, torch.nn.Module)\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.nn.modules.module.Module,\n",
       " torch.nn.modules.linear.Identity,\n",
       " torch.nn.modules.linear.Linear,\n",
       " torch.nn.modules.linear.Bilinear,\n",
       " torch.nn.modules.linear.LazyLinear,\n",
       " torch.nn.modules.conv.Conv1d,\n",
       " torch.nn.modules.conv.Conv2d,\n",
       " torch.nn.modules.conv.Conv3d,\n",
       " torch.nn.modules.conv.ConvTranspose1d,\n",
       " torch.nn.modules.conv.ConvTranspose2d,\n",
       " torch.nn.modules.conv.ConvTranspose3d,\n",
       " torch.nn.modules.conv.LazyConv1d,\n",
       " torch.nn.modules.conv.LazyConv2d,\n",
       " torch.nn.modules.conv.LazyConv3d,\n",
       " torch.nn.modules.conv.LazyConvTranspose1d,\n",
       " torch.nn.modules.conv.LazyConvTranspose2d,\n",
       " torch.nn.modules.conv.LazyConvTranspose3d,\n",
       " torch.nn.modules.activation.Threshold,\n",
       " torch.nn.modules.activation.ReLU,\n",
       " torch.nn.modules.activation.Hardtanh,\n",
       " torch.nn.modules.activation.ReLU6,\n",
       " torch.nn.modules.activation.Sigmoid,\n",
       " torch.nn.modules.activation.Tanh,\n",
       " torch.nn.modules.activation.Softmax,\n",
       " torch.nn.modules.activation.Softmax2d,\n",
       " torch.nn.modules.activation.LogSoftmax,\n",
       " torch.nn.modules.activation.ELU,\n",
       " torch.nn.modules.activation.SELU,\n",
       " torch.nn.modules.activation.CELU,\n",
       " torch.nn.modules.activation.GELU,\n",
       " torch.nn.modules.activation.Hardshrink,\n",
       " torch.nn.modules.activation.LeakyReLU,\n",
       " torch.nn.modules.activation.LogSigmoid,\n",
       " torch.nn.modules.activation.Softplus,\n",
       " torch.nn.modules.activation.Softshrink,\n",
       " torch.nn.modules.activation.MultiheadAttention,\n",
       " torch.nn.modules.activation.PReLU,\n",
       " torch.nn.modules.activation.Softsign,\n",
       " torch.nn.modules.activation.Softmin,\n",
       " torch.nn.modules.activation.Tanhshrink,\n",
       " torch.nn.modules.activation.RReLU,\n",
       " torch.nn.modules.activation.GLU,\n",
       " torch.nn.modules.activation.Hardsigmoid,\n",
       " torch.nn.modules.activation.Hardswish,\n",
       " torch.nn.modules.activation.SiLU,\n",
       " torch.nn.modules.activation.Mish,\n",
       " torch.nn.modules.loss.L1Loss,\n",
       " torch.nn.modules.loss.NLLLoss,\n",
       " torch.nn.modules.loss.KLDivLoss,\n",
       " torch.nn.modules.loss.MSELoss,\n",
       " torch.nn.modules.loss.BCELoss,\n",
       " torch.nn.modules.loss.BCEWithLogitsLoss,\n",
       " torch.nn.modules.loss.NLLLoss2d,\n",
       " torch.nn.modules.loss.CosineEmbeddingLoss,\n",
       " torch.nn.modules.loss.CTCLoss,\n",
       " torch.nn.modules.loss.HingeEmbeddingLoss,\n",
       " torch.nn.modules.loss.MarginRankingLoss,\n",
       " torch.nn.modules.loss.MultiLabelMarginLoss,\n",
       " torch.nn.modules.loss.MultiLabelSoftMarginLoss,\n",
       " torch.nn.modules.loss.MultiMarginLoss,\n",
       " torch.nn.modules.loss.SmoothL1Loss,\n",
       " torch.nn.modules.loss.HuberLoss,\n",
       " torch.nn.modules.loss.SoftMarginLoss,\n",
       " torch.nn.modules.loss.CrossEntropyLoss,\n",
       " torch.nn.modules.loss.TripletMarginLoss,\n",
       " torch.nn.modules.loss.TripletMarginWithDistanceLoss,\n",
       " torch.nn.modules.loss.PoissonNLLLoss,\n",
       " torch.nn.modules.loss.GaussianNLLLoss,\n",
       " torch.nn.modules.container.Container,\n",
       " torch.nn.modules.container.Sequential,\n",
       " torch.nn.modules.container.ModuleList,\n",
       " torch.nn.modules.container.ModuleDict,\n",
       " torch.nn.modules.container.ParameterList,\n",
       " torch.nn.modules.container.ParameterDict,\n",
       " torch.nn.modules.pooling.AvgPool1d,\n",
       " torch.nn.modules.pooling.AvgPool2d,\n",
       " torch.nn.modules.pooling.AvgPool3d,\n",
       " torch.nn.modules.pooling.MaxPool1d,\n",
       " torch.nn.modules.pooling.MaxPool2d,\n",
       " torch.nn.modules.pooling.MaxPool3d,\n",
       " torch.nn.modules.pooling.MaxUnpool1d,\n",
       " torch.nn.modules.pooling.MaxUnpool2d,\n",
       " torch.nn.modules.pooling.MaxUnpool3d,\n",
       " torch.nn.modules.pooling.FractionalMaxPool2d,\n",
       " torch.nn.modules.pooling.FractionalMaxPool3d,\n",
       " torch.nn.modules.pooling.LPPool1d,\n",
       " torch.nn.modules.pooling.LPPool2d,\n",
       " torch.nn.modules.pooling.AdaptiveMaxPool1d,\n",
       " torch.nn.modules.pooling.AdaptiveMaxPool2d,\n",
       " torch.nn.modules.pooling.AdaptiveMaxPool3d,\n",
       " torch.nn.modules.pooling.AdaptiveAvgPool1d,\n",
       " torch.nn.modules.pooling.AdaptiveAvgPool2d,\n",
       " torch.nn.modules.pooling.AdaptiveAvgPool3d,\n",
       " torch.nn.modules.batchnorm.BatchNorm1d,\n",
       " torch.nn.modules.batchnorm.BatchNorm2d,\n",
       " torch.nn.modules.batchnorm.BatchNorm3d,\n",
       " torch.nn.modules.batchnorm.SyncBatchNorm,\n",
       " torch.nn.modules.batchnorm.LazyBatchNorm1d,\n",
       " torch.nn.modules.batchnorm.LazyBatchNorm2d,\n",
       " torch.nn.modules.batchnorm.LazyBatchNorm3d,\n",
       " torch.nn.modules.instancenorm.InstanceNorm1d,\n",
       " torch.nn.modules.instancenorm.InstanceNorm2d,\n",
       " torch.nn.modules.instancenorm.InstanceNorm3d,\n",
       " torch.nn.modules.instancenorm.LazyInstanceNorm1d,\n",
       " torch.nn.modules.instancenorm.LazyInstanceNorm2d,\n",
       " torch.nn.modules.instancenorm.LazyInstanceNorm3d,\n",
       " torch.nn.modules.normalization.LocalResponseNorm,\n",
       " torch.nn.modules.normalization.CrossMapLRN2d,\n",
       " torch.nn.modules.normalization.LayerNorm,\n",
       " torch.nn.modules.normalization.GroupNorm,\n",
       " torch.nn.modules.dropout.Dropout,\n",
       " torch.nn.modules.dropout.Dropout1d,\n",
       " torch.nn.modules.dropout.Dropout2d,\n",
       " torch.nn.modules.dropout.Dropout3d,\n",
       " torch.nn.modules.dropout.AlphaDropout,\n",
       " torch.nn.modules.dropout.FeatureAlphaDropout,\n",
       " torch.nn.modules.padding.ReflectionPad1d,\n",
       " torch.nn.modules.padding.ReflectionPad2d,\n",
       " torch.nn.modules.padding.ReflectionPad3d,\n",
       " torch.nn.modules.padding.ReplicationPad1d,\n",
       " torch.nn.modules.padding.ReplicationPad2d,\n",
       " torch.nn.modules.padding.ReplicationPad3d,\n",
       " torch.nn.modules.padding.ZeroPad2d,\n",
       " torch.nn.modules.padding.ConstantPad1d,\n",
       " torch.nn.modules.padding.ConstantPad2d,\n",
       " torch.nn.modules.padding.ConstantPad3d,\n",
       " torch.nn.modules.sparse.Embedding,\n",
       " torch.nn.modules.sparse.EmbeddingBag,\n",
       " torch.nn.modules.rnn.RNNBase,\n",
       " torch.nn.modules.rnn.RNN,\n",
       " torch.nn.modules.rnn.LSTM,\n",
       " torch.nn.modules.rnn.GRU,\n",
       " torch.nn.modules.rnn.RNNCellBase,\n",
       " torch.nn.modules.rnn.RNNCell,\n",
       " torch.nn.modules.rnn.LSTMCell,\n",
       " torch.nn.modules.rnn.GRUCell,\n",
       " torch.nn.modules.pixelshuffle.PixelShuffle,\n",
       " torch.nn.modules.pixelshuffle.PixelUnshuffle,\n",
       " torch.nn.modules.upsampling.UpsamplingNearest2d,\n",
       " torch.nn.modules.upsampling.UpsamplingBilinear2d,\n",
       " torch.nn.modules.upsampling.Upsample,\n",
       " torch.nn.modules.distance.PairwiseDistance,\n",
       " torch.nn.modules.distance.CosineSimilarity,\n",
       " torch.nn.modules.fold.Fold,\n",
       " torch.nn.modules.fold.Unfold,\n",
       " torch.nn.modules.adaptive.AdaptiveLogSoftmaxWithLoss,\n",
       " torch.nn.modules.transformer.TransformerEncoder,\n",
       " torch.nn.modules.transformer.TransformerDecoder,\n",
       " torch.nn.modules.transformer.TransformerEncoderLayer,\n",
       " torch.nn.modules.transformer.TransformerDecoderLayer,\n",
       " torch.nn.modules.transformer.Transformer,\n",
       " torch.nn.modules.flatten.Flatten,\n",
       " torch.nn.modules.flatten.Unflatten,\n",
       " torch.nn.modules.channelshuffle.ChannelShuffle]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wishlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_norm = torch.nn.BatchNorm2d(3)\n",
    "layer_norm = torch.nn.LayerNorm((3,2))\n",
    "\n",
    "#instance_norm = torch.nn.InstanceNorm1d(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = torch.nn.Conv2d(3, 32, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[[-6.7824e-03,  6.8601e-02, -1.3309e-01],\n",
       "          [ 7.8424e-02,  2.9884e-02,  7.9124e-02],\n",
       "          [-6.9095e-02, -1.2547e-03,  3.3231e-02]],\n",
       "\n",
       "         [[ 1.1468e-01, -5.4619e-02,  9.4577e-02],\n",
       "          [ 1.1168e-01, -4.9108e-02, -1.1704e-01],\n",
       "          [ 1.5726e-01,  1.7837e-01, -3.6531e-02]],\n",
       "\n",
       "         [[-1.0974e-01, -1.7831e-01,  1.9031e-01],\n",
       "          [ 9.3539e-03, -1.1314e-01, -1.2883e-01],\n",
       "          [-1.1827e-01,  6.6672e-02,  1.2577e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 1.6878e-01, -7.9537e-04, -1.1808e-01],\n",
       "          [ 6.7397e-02,  1.8131e-01,  1.0973e-01],\n",
       "          [ 1.8140e-01, -8.7085e-02,  6.0301e-03]],\n",
       "\n",
       "         [[-3.3252e-02, -5.1587e-02, -1.3319e-01],\n",
       "          [-8.3417e-02,  9.7433e-02,  1.6584e-01],\n",
       "          [-3.2835e-02,  1.5325e-01,  6.5673e-02]],\n",
       "\n",
       "         [[ 1.1858e-01,  1.7501e-01,  1.7294e-01],\n",
       "          [-6.7541e-04, -3.0562e-02, -1.7066e-01],\n",
       "          [-1.8025e-01, -7.9931e-02, -1.8732e-01]]],\n",
       "\n",
       "\n",
       "        [[[-1.3819e-01,  1.1653e-01, -1.3730e-01],\n",
       "          [ 1.2744e-01,  8.8592e-03,  7.5464e-02],\n",
       "          [ 4.8189e-02, -1.5201e-01, -1.6645e-01]],\n",
       "\n",
       "         [[ 1.7983e-01,  1.2365e-01, -5.4263e-02],\n",
       "          [ 7.6195e-02,  9.4589e-02,  5.0225e-02],\n",
       "          [ 5.1728e-02,  5.9144e-02,  1.6185e-01]],\n",
       "\n",
       "         [[-1.2885e-01, -7.2838e-02, -9.2494e-02],\n",
       "          [ 1.1482e-01,  1.4528e-01,  1.2347e-01],\n",
       "          [ 8.2987e-02,  6.2627e-03, -1.1847e-01]]],\n",
       "\n",
       "\n",
       "        [[[-1.0978e-01,  1.1807e-01, -1.1489e-01],\n",
       "          [ 8.6551e-02, -1.6720e-01,  1.6591e-01],\n",
       "          [-6.5983e-02, -1.4745e-01,  1.4809e-01]],\n",
       "\n",
       "         [[-1.8989e-01, -1.4976e-01,  2.0735e-02],\n",
       "          [-4.1988e-02, -1.7499e-01, -1.6974e-01],\n",
       "          [ 9.3552e-02, -9.8029e-02, -1.1501e-02]],\n",
       "\n",
       "         [[ 1.8393e-01, -7.0675e-02,  6.4244e-02],\n",
       "          [-6.5719e-02,  1.3707e-01,  1.7922e-01],\n",
       "          [ 4.9237e-02, -6.3205e-02,  1.3879e-01]]],\n",
       "\n",
       "\n",
       "        [[[-1.4367e-01, -1.2267e-01, -1.2773e-01],\n",
       "          [ 1.0416e-01, -1.3962e-01, -3.1975e-02],\n",
       "          [ 8.1874e-02, -1.5231e-01,  9.9008e-02]],\n",
       "\n",
       "         [[-1.7880e-01, -1.8923e-03, -6.5087e-02],\n",
       "          [-1.8563e-01,  8.6156e-02, -1.3111e-01],\n",
       "          [ 1.8289e-01,  8.4980e-02, -1.6766e-01]],\n",
       "\n",
       "         [[ 1.5507e-01,  2.1823e-02, -1.6964e-01],\n",
       "          [-1.0874e-01,  1.2075e-01, -7.4374e-03],\n",
       "          [-7.1533e-03,  1.0244e-01, -1.0780e-03]]],\n",
       "\n",
       "\n",
       "        [[[ 1.7752e-02, -1.6564e-01,  1.8107e-01],\n",
       "          [-2.7906e-02,  1.1456e-01,  6.2399e-03],\n",
       "          [-7.1264e-02, -1.9038e-01,  1.7120e-01]],\n",
       "\n",
       "         [[-7.9949e-02,  5.4498e-02,  6.4196e-02],\n",
       "          [-1.4028e-01,  1.5243e-01, -1.8131e-01],\n",
       "          [ 7.2441e-02,  1.7388e-01,  4.0851e-02]],\n",
       "\n",
       "         [[ 2.4899e-02,  1.8426e-01,  1.7304e-01],\n",
       "          [ 1.5105e-01,  1.0081e-02,  1.0718e-01],\n",
       "          [-6.8407e-02, -1.1618e-01, -1.6785e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 1.4182e-01, -5.2695e-02, -1.6803e-01],\n",
       "          [-1.3605e-02, -1.0949e-01, -1.0187e-01],\n",
       "          [-9.6527e-02,  1.4525e-01, -9.4387e-02]],\n",
       "\n",
       "         [[ 1.8133e-01,  2.2404e-03, -4.3280e-02],\n",
       "          [-1.6910e-01, -9.4749e-02, -1.6607e-01],\n",
       "          [-1.2501e-01, -1.1648e-01,  1.2713e-01]],\n",
       "\n",
       "         [[-1.8076e-01,  9.4768e-02, -1.2880e-01],\n",
       "          [-1.7266e-02,  6.9543e-02, -1.3148e-01],\n",
       "          [-9.9622e-02,  1.8445e-01,  7.5042e-02]]],\n",
       "\n",
       "\n",
       "        [[[ 1.8981e-01, -1.6508e-01,  4.1556e-02],\n",
       "          [ 1.0649e-01, -9.8235e-02, -1.7604e-01],\n",
       "          [ 6.6007e-02,  1.6200e-01, -2.4999e-02]],\n",
       "\n",
       "         [[-1.2653e-01,  8.4036e-02, -1.0584e-01],\n",
       "          [-8.3074e-02,  1.1235e-01,  3.0595e-02],\n",
       "          [-1.9057e-01, -1.3828e-01,  1.9031e-01]],\n",
       "\n",
       "         [[-1.3405e-01, -6.8114e-02,  9.8542e-02],\n",
       "          [ 9.1024e-02, -1.0793e-01,  2.4526e-02],\n",
       "          [-9.6708e-02,  8.5215e-02,  9.2602e-02]]],\n",
       "\n",
       "\n",
       "        [[[-4.1079e-02, -9.4010e-02,  1.3212e-01],\n",
       "          [-1.6860e-02, -8.6922e-03,  4.7394e-03],\n",
       "          [ 5.5540e-02,  1.6885e-01,  1.3534e-01]],\n",
       "\n",
       "         [[ 1.2585e-01,  1.5396e-01, -5.5652e-02],\n",
       "          [ 1.8321e-01,  4.8571e-02, -4.0751e-02],\n",
       "          [-1.0568e-01,  1.5351e-01,  5.0786e-02]],\n",
       "\n",
       "         [[-1.7424e-01,  2.9275e-02,  7.9614e-02],\n",
       "          [-8.3583e-03,  8.7719e-02, -5.1057e-02],\n",
       "          [-1.8186e-01, -4.2706e-02, -6.2377e-03]]],\n",
       "\n",
       "\n",
       "        [[[-1.3594e-01,  1.1249e-02,  2.2529e-02],\n",
       "          [-1.8164e-01, -1.4481e-03,  1.8802e-01],\n",
       "          [-1.7619e-01,  1.7669e-01,  3.4484e-02]],\n",
       "\n",
       "         [[-4.6837e-02,  3.5707e-02,  9.1955e-02],\n",
       "          [ 1.4164e-01,  4.0536e-02, -9.2073e-02],\n",
       "          [ 1.5905e-02,  7.7630e-02, -1.5524e-01]],\n",
       "\n",
       "         [[-5.8731e-06, -7.6374e-03,  1.1806e-02],\n",
       "          [-1.7755e-01,  1.2689e-01,  1.1343e-01],\n",
       "          [-3.0000e-02, -2.3511e-02,  1.0834e-01]]],\n",
       "\n",
       "\n",
       "        [[[-3.5747e-02,  1.4206e-01,  7.6797e-02],\n",
       "          [ 1.7795e-01, -9.3355e-02,  1.2547e-01],\n",
       "          [-3.8414e-02,  2.2889e-02,  1.7905e-01]],\n",
       "\n",
       "         [[ 1.3681e-01, -8.0493e-02,  1.3088e-01],\n",
       "          [-1.8767e-01,  1.4145e-02,  1.1970e-01],\n",
       "          [-2.8215e-02, -8.3388e-03, -6.0745e-02]],\n",
       "\n",
       "         [[ 6.2702e-02,  3.0786e-02,  1.6942e-01],\n",
       "          [-5.9536e-02, -1.6045e-01,  1.5289e-01],\n",
       "          [ 1.4367e-01,  1.7698e-02, -1.2382e-02]]],\n",
       "\n",
       "\n",
       "        [[[ 6.9042e-03, -3.7593e-02, -2.7327e-02],\n",
       "          [ 9.9721e-02,  1.5467e-01,  1.0632e-01],\n",
       "          [ 6.3973e-02,  1.8586e-01,  8.0060e-02]],\n",
       "\n",
       "         [[ 6.1085e-02,  1.5025e-01,  6.1088e-02],\n",
       "          [-9.1708e-02, -7.6226e-02, -3.4283e-02],\n",
       "          [-7.2773e-02,  4.9829e-02, -1.1553e-01]],\n",
       "\n",
       "         [[-5.0448e-02,  1.0444e-01,  1.9057e-01],\n",
       "          [-7.3482e-02, -1.7997e-01,  1.4879e-01],\n",
       "          [ 5.5305e-02, -9.1958e-02,  1.6221e-01]]],\n",
       "\n",
       "\n",
       "        [[[-1.0921e-01,  5.9942e-03,  1.7526e-02],\n",
       "          [-1.0968e-01,  6.9101e-02,  1.3429e-01],\n",
       "          [-1.7487e-01, -1.3614e-01, -6.3109e-02]],\n",
       "\n",
       "         [[-2.8403e-02, -1.9104e-01,  9.4241e-02],\n",
       "          [ 4.5533e-02, -4.4604e-03,  1.7453e-01],\n",
       "          [ 1.0893e-01,  4.2619e-03,  1.8380e-01]],\n",
       "\n",
       "         [[ 1.7282e-01, -1.4921e-01,  7.9803e-02],\n",
       "          [ 6.7938e-02,  9.5469e-02,  1.4057e-01],\n",
       "          [ 5.0445e-02,  8.1342e-02,  9.5663e-02]]],\n",
       "\n",
       "\n",
       "        [[[-7.9374e-02,  7.0541e-02,  6.8029e-02],\n",
       "          [-1.3688e-01, -1.8354e-01,  1.5627e-01],\n",
       "          [-1.9197e-01,  1.8712e-01, -1.9231e-01]],\n",
       "\n",
       "         [[-1.8003e-01, -1.2250e-01,  5.7663e-02],\n",
       "          [-1.4553e-01,  1.8130e-01, -3.7237e-02],\n",
       "          [ 1.4790e-01,  1.8415e-01,  4.3464e-02]],\n",
       "\n",
       "         [[-4.7055e-03,  2.1305e-02, -1.4647e-01],\n",
       "          [-1.0703e-01,  1.2385e-01, -1.8007e-01],\n",
       "          [-1.2635e-01, -1.0149e-01, -1.9065e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 1.0739e-01,  4.5940e-02, -1.7340e-01],\n",
       "          [-1.0913e-01,  1.1106e-01, -1.3349e-01],\n",
       "          [ 2.5158e-02,  7.2106e-02,  1.3542e-01]],\n",
       "\n",
       "         [[ 1.3327e-01,  1.0444e-01, -1.8838e-01],\n",
       "          [-6.0573e-02,  1.2342e-01, -5.6579e-02],\n",
       "          [ 3.7186e-02,  1.9111e-01,  9.2943e-02]],\n",
       "\n",
       "         [[ 6.0493e-02,  1.3597e-01,  8.3834e-02],\n",
       "          [ 1.6481e-01,  9.7179e-02, -1.0620e-02],\n",
       "          [-1.0300e-01,  1.7225e-01, -2.2048e-02]]],\n",
       "\n",
       "\n",
       "        [[[-1.2905e-01, -1.4192e-01,  1.4470e-01],\n",
       "          [-5.6879e-02, -1.1281e-01,  1.2509e-01],\n",
       "          [ 1.4630e-01, -8.1455e-02, -1.3096e-01]],\n",
       "\n",
       "         [[-6.9272e-02,  1.4020e-01,  1.6306e-01],\n",
       "          [ 1.5561e-01,  1.0891e-01,  6.1126e-02],\n",
       "          [ 1.6392e-01, -1.6822e-01, -9.9196e-02]],\n",
       "\n",
       "         [[ 1.1986e-01, -2.8266e-03,  1.8013e-01],\n",
       "          [ 2.3276e-02, -3.1872e-02, -1.0371e-01],\n",
       "          [-1.0028e-02,  5.8979e-02,  1.9147e-01]]],\n",
       "\n",
       "\n",
       "        [[[-9.7122e-02,  1.5195e-01, -1.1334e-01],\n",
       "          [-1.2709e-01, -2.9596e-02, -1.5442e-01],\n",
       "          [ 1.6690e-01, -1.5278e-01,  1.5650e-01]],\n",
       "\n",
       "         [[-1.6538e-01,  8.8517e-03, -1.3353e-01],\n",
       "          [ 8.7872e-02, -1.1105e-02,  4.4090e-02],\n",
       "          [ 4.6583e-02, -6.2842e-02, -4.5256e-02]],\n",
       "\n",
       "         [[-3.9872e-02, -8.4181e-02, -7.9849e-02],\n",
       "          [-7.5544e-02,  4.4722e-02, -1.0951e-01],\n",
       "          [ 1.2849e-01, -3.2563e-02, -2.9937e-02]]],\n",
       "\n",
       "\n",
       "        [[[-9.3081e-02, -1.5855e-01,  1.6173e-01],\n",
       "          [-1.6768e-02,  1.8394e-01, -5.2176e-02],\n",
       "          [-5.0874e-02, -1.0459e-01,  6.7115e-02]],\n",
       "\n",
       "         [[-4.1292e-02, -1.6203e-02,  1.7668e-02],\n",
       "          [ 1.6908e-01, -4.5456e-02,  4.0689e-02],\n",
       "          [-1.7101e-01,  1.7317e-01,  1.8243e-01]],\n",
       "\n",
       "         [[-1.0911e-01,  1.4491e-01,  1.4901e-01],\n",
       "          [-2.5301e-02,  1.8577e-01,  1.4940e-01],\n",
       "          [-1.5323e-01,  6.9092e-02, -1.7714e-02]]],\n",
       "\n",
       "\n",
       "        [[[-5.4910e-02,  1.6419e-01, -3.4558e-02],\n",
       "          [ 1.0878e-01,  6.4581e-02, -1.2384e-01],\n",
       "          [-4.2251e-02, -8.0074e-02,  3.1714e-02]],\n",
       "\n",
       "         [[-5.3733e-03,  1.2233e-01,  1.2388e-01],\n",
       "          [-2.6753e-02, -1.9181e-01,  2.6715e-02],\n",
       "          [ 7.8008e-02,  5.8669e-02, -7.3092e-02]],\n",
       "\n",
       "         [[-9.8727e-02, -4.1875e-02, -1.3717e-01],\n",
       "          [ 8.0255e-02,  2.1463e-02,  1.7992e-01],\n",
       "          [ 1.8476e-01, -1.2000e-01,  1.2299e-01]]],\n",
       "\n",
       "\n",
       "        [[[-1.3511e-02, -1.7991e-01, -1.6173e-01],\n",
       "          [-6.8911e-03, -2.0914e-02,  1.1299e-01],\n",
       "          [ 1.3217e-01,  1.8631e-01,  1.8593e-01]],\n",
       "\n",
       "         [[ 1.6397e-01, -1.8509e-01, -1.8297e-01],\n",
       "          [-1.1021e-01,  6.4059e-02, -1.8119e-01],\n",
       "          [ 1.1906e-02,  1.6396e-01, -6.5969e-02]],\n",
       "\n",
       "         [[ 3.5718e-02,  4.0187e-02,  1.0168e-01],\n",
       "          [ 2.9206e-02,  1.3929e-01,  2.2522e-03],\n",
       "          [ 1.5265e-01, -6.7015e-02, -3.9479e-02]]],\n",
       "\n",
       "\n",
       "        [[[-9.1563e-02, -1.5455e-01, -2.8419e-02],\n",
       "          [-3.6920e-02, -1.7192e-01, -1.8315e-03],\n",
       "          [-8.1055e-02,  1.5112e-01,  1.3973e-01]],\n",
       "\n",
       "         [[-6.9347e-02,  1.6116e-01, -7.7840e-02],\n",
       "          [-9.2877e-02, -2.3642e-02,  2.1098e-02],\n",
       "          [-1.6969e-01, -7.7382e-02, -2.5711e-02]],\n",
       "\n",
       "         [[ 1.6500e-01,  1.8830e-02, -8.1313e-02],\n",
       "          [ 1.9425e-02, -1.0673e-02, -1.4581e-01],\n",
       "          [ 8.0627e-02,  1.8497e-01, -1.2624e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 7.2896e-02, -9.0042e-02, -1.1900e-01],\n",
       "          [-1.6725e-02, -4.0766e-02,  6.0821e-02],\n",
       "          [ 1.1519e-01,  1.1768e-02,  1.2871e-01]],\n",
       "\n",
       "         [[-1.3537e-01,  1.7061e-01, -1.0625e-01],\n",
       "          [-1.3139e-01,  9.0862e-02,  4.1775e-02],\n",
       "          [ 1.2418e-01,  1.5024e-01, -1.6637e-01]],\n",
       "\n",
       "         [[-4.0582e-02, -1.2227e-01, -4.8903e-03],\n",
       "          [ 1.4954e-01,  1.3478e-02,  3.9000e-02],\n",
       "          [ 3.9022e-02,  1.2731e-01,  8.0823e-02]]],\n",
       "\n",
       "\n",
       "        [[[-1.5105e-01,  3.6564e-02,  8.3921e-02],\n",
       "          [ 9.6605e-02,  1.3774e-01,  8.6788e-02],\n",
       "          [-2.2031e-02, -8.8808e-02, -1.7177e-01]],\n",
       "\n",
       "         [[-5.4552e-02,  3.5158e-02,  9.0907e-03],\n",
       "          [-1.2100e-01,  3.7660e-03,  1.7920e-01],\n",
       "          [-1.1896e-01, -7.4457e-02, -4.1863e-02]],\n",
       "\n",
       "         [[-5.2702e-02,  2.6124e-02,  3.3678e-02],\n",
       "          [-1.6647e-01, -6.8946e-02,  5.9193e-02],\n",
       "          [ 1.4149e-01,  1.7978e-01, -1.4498e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 1.8190e-02,  1.6711e-01,  1.0291e-01],\n",
       "          [-1.7828e-02,  1.3654e-01,  1.3637e-01],\n",
       "          [-1.2559e-02,  1.6300e-01, -1.0870e-01]],\n",
       "\n",
       "         [[ 1.2797e-01,  1.2868e-01, -2.2757e-02],\n",
       "          [-8.3082e-02,  7.7982e-02,  1.6948e-01],\n",
       "          [ 1.8791e-01, -1.8139e-01, -1.5135e-01]],\n",
       "\n",
       "         [[-1.7479e-01,  1.1944e-01, -1.6924e-01],\n",
       "          [ 1.8440e-01, -2.1916e-02, -1.1787e-01],\n",
       "          [ 4.3838e-02, -9.3092e-02, -8.8907e-02]]],\n",
       "\n",
       "\n",
       "        [[[ 5.5933e-02, -2.1775e-02,  6.2989e-02],\n",
       "          [ 1.0718e-01, -7.9694e-02,  1.6986e-01],\n",
       "          [-1.8209e-01,  1.6804e-01, -3.9765e-02]],\n",
       "\n",
       "         [[ 8.6027e-02, -7.7229e-02, -2.0517e-02],\n",
       "          [ 1.4906e-02,  7.1910e-02,  1.0768e-01],\n",
       "          [ 1.4105e-01,  6.8605e-02, -8.7941e-02]],\n",
       "\n",
       "         [[ 8.3858e-02, -3.0010e-02,  1.1121e-01],\n",
       "          [ 8.3465e-02, -8.2009e-02, -8.0133e-02],\n",
       "          [-2.8704e-02,  4.1892e-02,  1.7607e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 3.0570e-04,  1.3790e-01, -1.2418e-01],\n",
       "          [-8.0828e-02, -4.6476e-02, -1.9178e-01],\n",
       "          [ 1.7882e-01, -8.0705e-02, -1.5131e-01]],\n",
       "\n",
       "         [[ 2.5106e-02, -1.8668e-01,  1.0584e-03],\n",
       "          [ 1.5452e-01,  1.1327e-01, -1.6252e-01],\n",
       "          [ 5.7203e-02,  1.0811e-01, -1.6601e-01]],\n",
       "\n",
       "         [[ 1.4171e-01, -1.1573e-01, -1.1420e-01],\n",
       "          [-1.7027e-01, -4.8906e-02,  4.4673e-02],\n",
       "          [-8.3191e-02, -4.0117e-03,  1.0779e-01]]],\n",
       "\n",
       "\n",
       "        [[[-5.2842e-03,  1.3889e-01,  7.4189e-02],\n",
       "          [ 3.4879e-02,  1.1562e-01, -7.1723e-02],\n",
       "          [ 1.4259e-02,  1.2434e-01, -1.7064e-01]],\n",
       "\n",
       "         [[ 1.0762e-01, -1.6570e-01,  6.2496e-02],\n",
       "          [-2.0383e-02, -2.2135e-02, -1.5252e-01],\n",
       "          [ 1.6370e-01, -5.2404e-02,  1.2137e-01]],\n",
       "\n",
       "         [[ 5.6897e-02, -9.9789e-02, -8.4123e-02],\n",
       "          [-1.6277e-01,  1.6537e-01, -1.5943e-01],\n",
       "          [ 6.4474e-02,  1.0381e-01, -1.8832e-01]]],\n",
       "\n",
       "\n",
       "        [[[-7.3110e-02, -8.4328e-02, -9.4523e-02],\n",
       "          [-6.0729e-02, -1.6310e-01,  1.1136e-01],\n",
       "          [ 8.8264e-02,  1.0665e-02, -1.8207e-01]],\n",
       "\n",
       "         [[ 3.6817e-02, -7.7388e-03, -1.7807e-01],\n",
       "          [-1.5805e-01,  1.4381e-01,  7.8444e-02],\n",
       "          [-3.1160e-02, -4.5202e-02,  1.1962e-01]],\n",
       "\n",
       "         [[ 1.5187e-01,  8.7291e-02, -9.4879e-03],\n",
       "          [-3.0308e-03, -1.6663e-01,  7.0190e-02],\n",
       "          [ 7.0106e-02,  8.9290e-02, -8.5137e-02]]],\n",
       "\n",
       "\n",
       "        [[[ 8.1145e-02, -1.6548e-01,  8.4061e-02],\n",
       "          [-1.1609e-01, -1.1382e-01,  1.9095e-02],\n",
       "          [ 1.4112e-01, -1.6463e-01,  1.7903e-01]],\n",
       "\n",
       "         [[-1.5349e-01,  1.7041e-01, -9.8102e-02],\n",
       "          [-1.4141e-01,  4.4208e-02, -1.4371e-01],\n",
       "          [-2.4122e-03, -7.9942e-02, -1.8252e-01]],\n",
       "\n",
       "         [[-1.2617e-01, -4.1271e-02, -1.3230e-01],\n",
       "          [ 1.6582e-01, -1.5051e-01,  2.6195e-02],\n",
       "          [-5.5316e-02, -8.0719e-02,  1.0981e-01]]],\n",
       "\n",
       "\n",
       "        [[[-1.7055e-01, -9.5337e-02,  4.1079e-02],\n",
       "          [ 1.7847e-02, -2.9198e-02,  8.9757e-03],\n",
       "          [ 1.1729e-01,  7.3159e-02, -1.4558e-01]],\n",
       "\n",
       "         [[-5.1095e-02,  1.9011e-01,  1.8116e-01],\n",
       "          [ 3.4778e-02, -1.1495e-01, -1.6607e-01],\n",
       "          [ 1.2827e-02, -2.4931e-02,  3.7042e-02]],\n",
       "\n",
       "         [[ 8.9991e-03,  9.3882e-02,  1.2384e-01],\n",
       "          [ 1.1603e-01,  1.6848e-01,  1.9939e-02],\n",
       "          [-8.8415e-02, -2.9741e-02,  1.2755e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 1.8524e-01,  1.7109e-01, -1.4881e-01],\n",
       "          [ 5.7126e-03,  6.9675e-02,  1.6168e-01],\n",
       "          [ 3.1578e-02,  2.4197e-02,  5.8736e-02]],\n",
       "\n",
       "         [[ 2.0764e-02,  1.0695e-01, -3.8140e-02],\n",
       "          [ 1.4691e-01, -1.7105e-01, -5.5955e-02],\n",
       "          [-3.0423e-02,  9.1373e-02,  1.1940e-01]],\n",
       "\n",
       "         [[ 1.3047e-01,  3.9819e-02, -1.8469e-01],\n",
       "          [ 2.1601e-02,  1.8106e-01,  6.0254e-02],\n",
       "          [-3.4848e-02,  1.2571e-01, -1.7922e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 1.6042e-01,  8.8365e-02, -5.2442e-02],\n",
       "          [-1.4394e-01, -5.1076e-02, -1.4872e-01],\n",
       "          [ 1.5694e-01,  1.0439e-01,  3.6102e-03]],\n",
       "\n",
       "         [[-6.7983e-03,  6.8081e-02, -7.1126e-02],\n",
       "          [-1.1379e-01, -3.6780e-02,  1.3892e-01],\n",
       "          [-1.3515e-01,  4.0093e-02,  1.7243e-02]],\n",
       "\n",
       "         [[ 9.7259e-02, -1.7254e-01, -1.0833e-02],\n",
       "          [ 7.9680e-02,  1.2235e-01, -5.5191e-02],\n",
       "          [-1.9203e-03, -1.0991e-02,  5.2460e-02]]]], requires_grad=True)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-0.1171,  0.0065, -0.1017,  0.0967,  0.0845,  0.0323,  0.0955, -0.0843,\n",
       "         0.0185, -0.0223,  0.0586,  0.0553, -0.0800,  0.0098,  0.0280, -0.0265,\n",
       "         0.0949, -0.0013,  0.0646, -0.1371,  0.1145,  0.0042, -0.1288,  0.1031,\n",
       "         0.1849, -0.1484, -0.1718, -0.1155, -0.0631,  0.1688,  0.1342,  0.0402],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['weight', 'bias'])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv._parameters.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['training', '_parameters', '_buffers', '_non_persistent_buffers_set', '_backward_hooks', '_is_full_backward_hook', '_forward_hooks', '_forward_pre_hooks', '_state_dict_hooks', '_load_state_dict_pre_hooks', '_load_state_dict_post_hooks', '_modules', 'in_channels', 'out_channels', 'kernel_size', 'stride', 'padding', 'dilation', 'transposed', 'output_padding', 'groups', 'padding_mode', '_reversed_padding_repeated_twice'])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv.__dict__.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mInit signature:\u001b[0m\n",
      "\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0min_channels\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mout_channels\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mkernel_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mstride\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mpadding\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdilation\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mgroups\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mbias\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mpadding_mode\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'zeros'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m     \n",
      "Applies a 2D convolution over an input signal composed of several input\n",
      "planes.\n",
      "\n",
      "In the simplest case, the output value of the layer with input size\n",
      ":math:`(N, C_{\\text{in}}, H, W)` and output :math:`(N, C_{\\text{out}}, H_{\\text{out}}, W_{\\text{out}})`\n",
      "can be precisely described as:\n",
      "\n",
      ".. math::\n",
      "    \\text{out}(N_i, C_{\\text{out}_j}) = \\text{bias}(C_{\\text{out}_j}) +\n",
      "    \\sum_{k = 0}^{C_{\\text{in}} - 1} \\text{weight}(C_{\\text{out}_j}, k) \\star \\text{input}(N_i, k)\n",
      "\n",
      "\n",
      "where :math:`\\star` is the valid 2D `cross-correlation`_ operator,\n",
      ":math:`N` is a batch size, :math:`C` denotes a number of channels,\n",
      ":math:`H` is a height of input planes in pixels, and :math:`W` is\n",
      "width in pixels.\n",
      "\n",
      "\n",
      "This module supports :ref:`TensorFloat32<tf32_on_ampere>`.\n",
      "\n",
      "On certain ROCm devices, when using float16 inputs this module will use :ref:`different precision<fp16_on_mi200>` for backward.\n",
      "\n",
      "* :attr:`stride` controls the stride for the cross-correlation, a single\n",
      "  number or a tuple.\n",
      "\n",
      "* :attr:`padding` controls the amount of padding applied to the input. It\n",
      "  can be either a string {'valid', 'same'} or a tuple of ints giving the\n",
      "  amount of implicit padding applied on both sides.\n",
      "\n",
      "* :attr:`dilation` controls the spacing between the kernel points; also\n",
      "  known as the à trous algorithm. It is harder to describe, but this `link`_\n",
      "  has a nice visualization of what :attr:`dilation` does.\n",
      "\n",
      "* :attr:`groups` controls the connections between inputs and outputs.\n",
      "  :attr:`in_channels` and :attr:`out_channels` must both be divisible by\n",
      "  :attr:`groups`. For example,\n",
      "\n",
      "    * At groups=1, all inputs are convolved to all outputs.\n",
      "    * At groups=2, the operation becomes equivalent to having two conv\n",
      "      layers side by side, each seeing half the input channels\n",
      "      and producing half the output channels, and both subsequently\n",
      "      concatenated.\n",
      "    * At groups= :attr:`in_channels`, each input channel is convolved with\n",
      "      its own set of filters (of size\n",
      "      :math:`\\frac{\\text{out\\_channels}}{\\text{in\\_channels}}`).\n",
      "\n",
      "The parameters :attr:`kernel_size`, :attr:`stride`, :attr:`padding`, :attr:`dilation` can either be:\n",
      "\n",
      "    - a single ``int`` -- in which case the same value is used for the height and width dimension\n",
      "    - a ``tuple`` of two ints -- in which case, the first `int` is used for the height dimension,\n",
      "      and the second `int` for the width dimension\n",
      "\n",
      "Note:\n",
      "    When `groups == in_channels` and `out_channels == K * in_channels`,\n",
      "    where `K` is a positive integer, this operation is also known as a \"depthwise convolution\".\n",
      "\n",
      "    In other words, for an input of size :math:`(N, C_{in}, L_{in})`,\n",
      "    a depthwise convolution with a depthwise multiplier `K` can be performed with the arguments\n",
      "    :math:`(C_\\text{in}=C_\\text{in}, C_\\text{out}=C_\\text{in} \\times \\text{K}, ..., \\text{groups}=C_\\text{in})`.\n",
      "\n",
      "Note:\n",
      "    In some circumstances when given tensors on a CUDA device and using CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting ``torch.backends.cudnn.deterministic = True``. See :doc:`/notes/randomness` for more information.\n",
      "\n",
      "Note:\n",
      "    ``padding='valid'`` is the same as no padding. ``padding='same'`` pads\n",
      "    the input so the output has the shape as the input. However, this mode\n",
      "    doesn't support any stride values other than 1.\n",
      "\n",
      "Note:\n",
      "    This module supports complex data types i.e. ``complex32, complex64, complex128``.\n",
      "\n",
      "Args:\n",
      "    in_channels (int): Number of channels in the input image\n",
      "    out_channels (int): Number of channels produced by the convolution\n",
      "    kernel_size (int or tuple): Size of the convolving kernel\n",
      "    stride (int or tuple, optional): Stride of the convolution. Default: 1\n",
      "    padding (int, tuple or str, optional): Padding added to all four sides of\n",
      "        the input. Default: 0\n",
      "    padding_mode (string, optional): ``'zeros'``, ``'reflect'``,\n",
      "        ``'replicate'`` or ``'circular'``. Default: ``'zeros'``\n",
      "    dilation (int or tuple, optional): Spacing between kernel elements. Default: 1\n",
      "    groups (int, optional): Number of blocked connections from input\n",
      "        channels to output channels. Default: 1\n",
      "    bias (bool, optional): If ``True``, adds a learnable bias to the\n",
      "        output. Default: ``True``\n",
      "\n",
      "\n",
      "Shape:\n",
      "    - Input: :math:`(N, C_{in}, H_{in}, W_{in})` or :math:`(C_{in}, H_{in}, W_{in})`\n",
      "    - Output: :math:`(N, C_{out}, H_{out}, W_{out})` or :math:`(C_{out}, H_{out}, W_{out})`, where\n",
      "\n",
      "      .. math::\n",
      "          H_{out} = \\left\\lfloor\\frac{H_{in}  + 2 \\times \\text{padding}[0] - \\text{dilation}[0]\n",
      "                    \\times (\\text{kernel\\_size}[0] - 1) - 1}{\\text{stride}[0]} + 1\\right\\rfloor\n",
      "\n",
      "      .. math::\n",
      "          W_{out} = \\left\\lfloor\\frac{W_{in}  + 2 \\times \\text{padding}[1] - \\text{dilation}[1]\n",
      "                    \\times (\\text{kernel\\_size}[1] - 1) - 1}{\\text{stride}[1]} + 1\\right\\rfloor\n",
      "\n",
      "Attributes:\n",
      "    weight (Tensor): the learnable weights of the module of shape\n",
      "        :math:`(\\text{out\\_channels}, \\frac{\\text{in\\_channels}}{\\text{groups}},`\n",
      "        :math:`\\text{kernel\\_size[0]}, \\text{kernel\\_size[1]})`.\n",
      "        The values of these weights are sampled from\n",
      "        :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})` where\n",
      "        :math:`k = \\frac{groups}{C_\\text{in} * \\prod_{i=0}^{1}\\text{kernel\\_size}[i]}`\n",
      "    bias (Tensor):   the learnable bias of the module of shape\n",
      "        (out_channels). If :attr:`bias` is ``True``,\n",
      "        then the values of these weights are\n",
      "        sampled from :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})` where\n",
      "        :math:`k = \\frac{groups}{C_\\text{in} * \\prod_{i=0}^{1}\\text{kernel\\_size}[i]}`\n",
      "\n",
      "Examples:\n",
      "\n",
      "    >>> # With square kernels and equal stride\n",
      "    >>> m = nn.Conv2d(16, 33, 3, stride=2)\n",
      "    >>> # non-square kernels and unequal stride and with padding\n",
      "    >>> m = nn.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2))\n",
      "    >>> # non-square kernels and unequal stride and with padding and dilation\n",
      "    >>> m = nn.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2), dilation=(3, 1))\n",
      "    >>> input = torch.randn(20, 16, 50, 100)\n",
      "    >>> output = m(input)\n",
      "\n",
      ".. _cross-correlation:\n",
      "    https://en.wikipedia.org/wiki/Cross-correlation\n",
      "\n",
      ".. _link:\n",
      "    https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md\n",
      "\u001b[0;31mInit docstring:\u001b[0m Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
      "\u001b[0;31mFile:\u001b[0m           ~/.local/lib/python3.9/site-packages/torch/nn/modules/conv.py\n",
      "\u001b[0;31mType:\u001b[0m           type\n",
      "\u001b[0;31mSubclasses:\u001b[0m     LazyConv2d, Conv2d, ConvBn2d, Conv2d\n"
     ]
    }
   ],
   "source": [
    "torch.nn.Conv2d?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm.num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.nn.modules.module.Module'>\n",
      "<class 'torch.nn.modules.linear.Identity'>\n",
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "<class 'torch.nn.modules.conv.Conv1d'>\n",
      "<class 'torch.nn.modules.conv.Conv2d'>\n",
      "<class 'torch.nn.modules.conv.Conv3d'>\n",
      "<class 'torch.nn.modules.conv.ConvTranspose1d'>\n",
      "<class 'torch.nn.modules.conv.ConvTranspose2d'>\n",
      "<class 'torch.nn.modules.conv.ConvTranspose3d'>\n",
      "<class 'torch.nn.modules.activation.Threshold'>\n",
      "<class 'torch.nn.modules.activation.ReLU'>\n",
      "<class 'torch.nn.modules.activation.Hardtanh'>\n",
      "<class 'torch.nn.modules.activation.ReLU6'>\n",
      "<class 'torch.nn.modules.activation.Sigmoid'>\n",
      "<class 'torch.nn.modules.activation.Tanh'>\n",
      "<class 'torch.nn.modules.activation.Softmax'>\n",
      "<class 'torch.nn.modules.activation.Softmax2d'>\n",
      "<class 'torch.nn.modules.activation.LogSoftmax'>\n",
      "<class 'torch.nn.modules.activation.ELU'>\n",
      "<class 'torch.nn.modules.activation.SELU'>\n",
      "<class 'torch.nn.modules.activation.CELU'>\n",
      "<class 'torch.nn.modules.activation.GLU'>\n",
      "<class 'torch.nn.modules.activation.GELU'>\n",
      "<class 'torch.nn.modules.activation.Hardshrink'>\n",
      "<class 'torch.nn.modules.activation.LeakyReLU'>\n",
      "<class 'torch.nn.modules.activation.LogSigmoid'>\n",
      "<class 'torch.nn.modules.activation.Softplus'>\n",
      "<class 'torch.nn.modules.activation.Softshrink'>\n",
      "<class 'torch.nn.modules.activation.MultiheadAttention'>\n",
      "<class 'torch.nn.modules.activation.PReLU'>\n",
      "<class 'torch.nn.modules.activation.Softsign'>\n",
      "<class 'torch.nn.modules.activation.Softmin'>\n",
      "<class 'torch.nn.modules.activation.Tanhshrink'>\n",
      "<class 'torch.nn.modules.activation.RReLU'>\n",
      "<class 'torch.nn.modules.loss.L1Loss'>\n",
      "<class 'torch.nn.modules.loss.NLLLoss'>\n",
      "<class 'torch.nn.modules.loss.KLDivLoss'>\n",
      "<class 'torch.nn.modules.loss.MSELoss'>\n",
      "<class 'torch.nn.modules.loss.BCELoss'>\n",
      "<class 'torch.nn.modules.loss.BCEWithLogitsLoss'>\n",
      "<class 'torch.nn.modules.loss.NLLLoss2d'>\n",
      "<class 'torch.nn.modules.loss.PoissonNLLLoss'>\n",
      "<class 'torch.nn.modules.loss.CosineEmbeddingLoss'>\n",
      "<class 'torch.nn.modules.loss.CTCLoss'>\n",
      "<class 'torch.nn.modules.loss.HingeEmbeddingLoss'>\n",
      "<class 'torch.nn.modules.loss.MarginRankingLoss'>\n",
      "<class 'torch.nn.modules.loss.MultiLabelMarginLoss'>\n",
      "<class 'torch.nn.modules.loss.MultiLabelSoftMarginLoss'>\n",
      "<class 'torch.nn.modules.loss.MultiMarginLoss'>\n",
      "<class 'torch.nn.modules.loss.SmoothL1Loss'>\n",
      "<class 'torch.nn.modules.loss.GaussianNLLLoss'>\n",
      "<class 'torch.nn.modules.loss.HuberLoss'>\n",
      "<class 'torch.nn.modules.loss.SoftMarginLoss'>\n",
      "<class 'torch.nn.modules.loss.CrossEntropyLoss'>\n",
      "<class 'torch.nn.modules.container.Container'>\n",
      "<class 'torch.nn.modules.container.Sequential'>\n",
      "<class 'torch.nn.modules.container.ModuleList'>\n",
      "<class 'torch.nn.modules.container.ModuleDict'>\n",
      "<class 'torch.nn.modules.container.ParameterList'>\n",
      "<class 'torch.nn.modules.container.ParameterDict'>\n",
      "<class 'torch.nn.modules.pooling.AvgPool1d'>\n",
      "<class 'torch.nn.modules.pooling.AvgPool2d'>\n",
      "<class 'torch.nn.modules.pooling.AvgPool3d'>\n",
      "<class 'torch.nn.modules.pooling.MaxPool1d'>\n",
      "<class 'torch.nn.modules.pooling.MaxPool2d'>\n",
      "<class 'torch.nn.modules.pooling.MaxPool3d'>\n",
      "<class 'torch.nn.modules.pooling.MaxUnpool1d'>\n",
      "<class 'torch.nn.modules.pooling.MaxUnpool2d'>\n",
      "<class 'torch.nn.modules.pooling.MaxUnpool3d'>\n",
      "<class 'torch.nn.modules.pooling.FractionalMaxPool2d'>\n",
      "<class 'torch.nn.modules.pooling.FractionalMaxPool3d'>\n",
      "<class 'torch.nn.modules.pooling.LPPool1d'>\n",
      "<class 'torch.nn.modules.pooling.LPPool2d'>\n",
      "<class 'torch.nn.modules.normalization.LocalResponseNorm'>\n",
      "<class 'torch.nn.modules.batchnorm.BatchNorm1d'>\n",
      "<class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "<class 'torch.nn.modules.batchnorm.BatchNorm3d'>\n",
      "<class 'torch.nn.modules.instancenorm.InstanceNorm1d'>\n",
      "<class 'torch.nn.modules.instancenorm.InstanceNorm2d'>\n",
      "<class 'torch.nn.modules.instancenorm.InstanceNorm3d'>\n",
      "<class 'torch.nn.modules.normalization.LayerNorm'>\n",
      "<class 'torch.nn.modules.normalization.GroupNorm'>\n",
      "<class 'torch.nn.modules.batchnorm.SyncBatchNorm'>\n",
      "<class 'torch.nn.modules.dropout.Dropout'>\n",
      "<class 'torch.nn.modules.dropout.Dropout1d'>\n",
      "<class 'torch.nn.modules.dropout.Dropout2d'>\n",
      "<class 'torch.nn.modules.dropout.Dropout3d'>\n",
      "<class 'torch.nn.modules.dropout.AlphaDropout'>\n",
      "<class 'torch.nn.modules.dropout.FeatureAlphaDropout'>\n",
      "<class 'torch.nn.modules.padding.ReflectionPad1d'>\n",
      "<class 'torch.nn.modules.padding.ReflectionPad2d'>\n",
      "<class 'torch.nn.modules.padding.ReflectionPad3d'>\n",
      "<class 'torch.nn.modules.padding.ReplicationPad2d'>\n",
      "<class 'torch.nn.modules.padding.ReplicationPad1d'>\n",
      "<class 'torch.nn.modules.padding.ReplicationPad3d'>\n",
      "<class 'torch.nn.modules.normalization.CrossMapLRN2d'>\n",
      "<class 'torch.nn.modules.sparse.Embedding'>\n",
      "<class 'torch.nn.modules.sparse.EmbeddingBag'>\n",
      "<class 'torch.nn.modules.rnn.RNNBase'>\n",
      "<class 'torch.nn.modules.rnn.RNN'>\n",
      "<class 'torch.nn.modules.rnn.LSTM'>\n",
      "<class 'torch.nn.modules.rnn.GRU'>\n",
      "<class 'torch.nn.modules.rnn.RNNCellBase'>\n",
      "<class 'torch.nn.modules.rnn.RNNCell'>\n",
      "<class 'torch.nn.modules.rnn.LSTMCell'>\n",
      "<class 'torch.nn.modules.rnn.GRUCell'>\n",
      "<class 'torch.nn.modules.pixelshuffle.PixelShuffle'>\n",
      "<class 'torch.nn.modules.pixelshuffle.PixelUnshuffle'>\n",
      "<class 'torch.nn.modules.upsampling.Upsample'>\n",
      "<class 'torch.nn.modules.upsampling.UpsamplingNearest2d'>\n",
      "<class 'torch.nn.modules.upsampling.UpsamplingBilinear2d'>\n",
      "<class 'torch.nn.modules.distance.PairwiseDistance'>\n",
      "<class 'torch.nn.modules.pooling.AdaptiveMaxPool1d'>\n",
      "<class 'torch.nn.modules.pooling.AdaptiveMaxPool2d'>\n",
      "<class 'torch.nn.modules.pooling.AdaptiveMaxPool3d'>\n",
      "<class 'torch.nn.modules.pooling.AdaptiveAvgPool1d'>\n",
      "<class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>\n",
      "<class 'torch.nn.modules.pooling.AdaptiveAvgPool3d'>\n",
      "<class 'torch.nn.modules.loss.TripletMarginLoss'>\n",
      "<class 'torch.nn.modules.padding.ZeroPad2d'>\n",
      "<class 'torch.nn.modules.padding.ConstantPad1d'>\n",
      "<class 'torch.nn.modules.padding.ConstantPad2d'>\n",
      "<class 'torch.nn.modules.padding.ConstantPad3d'>\n",
      "<class 'torch.nn.modules.linear.Bilinear'>\n",
      "<class 'torch.nn.modules.distance.CosineSimilarity'>\n",
      "<class 'torch.nn.modules.fold.Unfold'>\n",
      "<class 'torch.nn.modules.fold.Fold'>\n",
      "<class 'torch.nn.modules.adaptive.AdaptiveLogSoftmaxWithLoss'>\n",
      "<class 'torch.nn.modules.transformer.TransformerEncoder'>\n",
      "<class 'torch.nn.modules.transformer.TransformerDecoder'>\n",
      "<class 'torch.nn.modules.transformer.TransformerEncoderLayer'>\n",
      "<class 'torch.nn.modules.transformer.TransformerDecoderLayer'>\n",
      "<class 'torch.nn.modules.transformer.Transformer'>\n",
      "<class 'torch.nn.modules.linear.LazyLinear'>\n",
      "<class 'torch.nn.modules.conv.LazyConv1d'>\n",
      "<class 'torch.nn.modules.conv.LazyConv2d'>\n",
      "<class 'torch.nn.modules.conv.LazyConv3d'>\n",
      "<class 'torch.nn.modules.conv.LazyConvTranspose1d'>\n",
      "<class 'torch.nn.modules.conv.LazyConvTranspose2d'>\n",
      "<class 'torch.nn.modules.conv.LazyConvTranspose3d'>\n",
      "<class 'torch.nn.modules.batchnorm.LazyBatchNorm1d'>\n",
      "<class 'torch.nn.modules.batchnorm.LazyBatchNorm2d'>\n",
      "<class 'torch.nn.modules.batchnorm.LazyBatchNorm3d'>\n",
      "<class 'torch.nn.modules.instancenorm.LazyInstanceNorm1d'>\n",
      "<class 'torch.nn.modules.instancenorm.LazyInstanceNorm2d'>\n",
      "<class 'torch.nn.modules.instancenorm.LazyInstanceNorm3d'>\n",
      "<class 'torch.nn.modules.flatten.Flatten'>\n",
      "<class 'torch.nn.modules.flatten.Unflatten'>\n",
      "<class 'torch.nn.modules.activation.Hardsigmoid'>\n",
      "<class 'torch.nn.modules.activation.Hardswish'>\n",
      "<class 'torch.nn.modules.activation.SiLU'>\n",
      "<class 'torch.nn.modules.activation.Mish'>\n",
      "<class 'torch.nn.modules.loss.TripletMarginWithDistanceLoss'>\n",
      "<class 'torch.nn.modules.channelshuffle.ChannelShuffle'>\n",
      "<class 'torch.nn.parallel.data_parallel.DataParallel'>\n"
     ]
    }
   ],
   "source": [
    "for key, value in torch.nn.__dict__.items():\n",
    "\n",
    "    if isinstance(value, type) and issubclass(value, torch.nn.Module):\n",
    "        print(str(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__name__',\n",
       " '__doc__',\n",
       " '__package__',\n",
       " '__loader__',\n",
       " '__spec__',\n",
       " '__path__',\n",
       " '__file__',\n",
       " '__cached__',\n",
       " '__builtins__',\n",
       " 'parameter',\n",
       " '_reduction',\n",
       " 'grad',\n",
       " 'functional',\n",
       " 'init',\n",
       " 'common_types',\n",
       " 'utils',\n",
       " 'modules',\n",
       " 'Module',\n",
       " 'Identity',\n",
       " 'Linear',\n",
       " 'Conv1d',\n",
       " 'Conv2d',\n",
       " 'Conv3d',\n",
       " 'ConvTranspose1d',\n",
       " 'ConvTranspose2d',\n",
       " 'ConvTranspose3d',\n",
       " 'Threshold',\n",
       " 'ReLU',\n",
       " 'Hardtanh',\n",
       " 'ReLU6',\n",
       " 'Sigmoid',\n",
       " 'Tanh',\n",
       " 'Softmax',\n",
       " 'Softmax2d',\n",
       " 'LogSoftmax',\n",
       " 'ELU',\n",
       " 'SELU',\n",
       " 'CELU',\n",
       " 'GLU',\n",
       " 'GELU',\n",
       " 'Hardshrink',\n",
       " 'LeakyReLU',\n",
       " 'LogSigmoid',\n",
       " 'Softplus',\n",
       " 'Softshrink',\n",
       " 'MultiheadAttention',\n",
       " 'PReLU',\n",
       " 'Softsign',\n",
       " 'Softmin',\n",
       " 'Tanhshrink',\n",
       " 'RReLU',\n",
       " 'L1Loss',\n",
       " 'NLLLoss',\n",
       " 'KLDivLoss',\n",
       " 'MSELoss',\n",
       " 'BCELoss',\n",
       " 'BCEWithLogitsLoss',\n",
       " 'NLLLoss2d',\n",
       " 'PoissonNLLLoss',\n",
       " 'CosineEmbeddingLoss',\n",
       " 'CTCLoss',\n",
       " 'HingeEmbeddingLoss',\n",
       " 'MarginRankingLoss',\n",
       " 'MultiLabelMarginLoss',\n",
       " 'MultiLabelSoftMarginLoss',\n",
       " 'MultiMarginLoss',\n",
       " 'SmoothL1Loss',\n",
       " 'GaussianNLLLoss',\n",
       " 'HuberLoss',\n",
       " 'SoftMarginLoss',\n",
       " 'CrossEntropyLoss',\n",
       " 'Container',\n",
       " 'Sequential',\n",
       " 'ModuleList',\n",
       " 'ModuleDict',\n",
       " 'ParameterList',\n",
       " 'ParameterDict',\n",
       " 'AvgPool1d',\n",
       " 'AvgPool2d',\n",
       " 'AvgPool3d',\n",
       " 'MaxPool1d',\n",
       " 'MaxPool2d',\n",
       " 'MaxPool3d',\n",
       " 'MaxUnpool1d',\n",
       " 'MaxUnpool2d',\n",
       " 'MaxUnpool3d',\n",
       " 'FractionalMaxPool2d',\n",
       " 'FractionalMaxPool3d',\n",
       " 'LPPool1d',\n",
       " 'LPPool2d',\n",
       " 'LocalResponseNorm',\n",
       " 'BatchNorm1d',\n",
       " 'BatchNorm2d',\n",
       " 'BatchNorm3d',\n",
       " 'InstanceNorm1d',\n",
       " 'InstanceNorm2d',\n",
       " 'InstanceNorm3d',\n",
       " 'LayerNorm',\n",
       " 'GroupNorm',\n",
       " 'SyncBatchNorm',\n",
       " 'Dropout',\n",
       " 'Dropout1d',\n",
       " 'Dropout2d',\n",
       " 'Dropout3d',\n",
       " 'AlphaDropout',\n",
       " 'FeatureAlphaDropout',\n",
       " 'ReflectionPad1d',\n",
       " 'ReflectionPad2d',\n",
       " 'ReflectionPad3d',\n",
       " 'ReplicationPad2d',\n",
       " 'ReplicationPad1d',\n",
       " 'ReplicationPad3d',\n",
       " 'CrossMapLRN2d',\n",
       " 'Embedding',\n",
       " 'EmbeddingBag',\n",
       " 'RNNBase',\n",
       " 'RNN',\n",
       " 'LSTM',\n",
       " 'GRU',\n",
       " 'RNNCellBase',\n",
       " 'RNNCell',\n",
       " 'LSTMCell',\n",
       " 'GRUCell',\n",
       " 'PixelShuffle',\n",
       " 'PixelUnshuffle',\n",
       " 'Upsample',\n",
       " 'UpsamplingNearest2d',\n",
       " 'UpsamplingBilinear2d',\n",
       " 'PairwiseDistance',\n",
       " 'AdaptiveMaxPool1d',\n",
       " 'AdaptiveMaxPool2d',\n",
       " 'AdaptiveMaxPool3d',\n",
       " 'AdaptiveAvgPool1d',\n",
       " 'AdaptiveAvgPool2d',\n",
       " 'AdaptiveAvgPool3d',\n",
       " 'TripletMarginLoss',\n",
       " 'ZeroPad2d',\n",
       " 'ConstantPad1d',\n",
       " 'ConstantPad2d',\n",
       " 'ConstantPad3d',\n",
       " 'Bilinear',\n",
       " 'CosineSimilarity',\n",
       " 'Unfold',\n",
       " 'Fold',\n",
       " 'AdaptiveLogSoftmaxWithLoss',\n",
       " 'TransformerEncoder',\n",
       " 'TransformerDecoder',\n",
       " 'TransformerEncoderLayer',\n",
       " 'TransformerDecoderLayer',\n",
       " 'Transformer',\n",
       " 'LazyLinear',\n",
       " 'LazyConv1d',\n",
       " 'LazyConv2d',\n",
       " 'LazyConv3d',\n",
       " 'LazyConvTranspose1d',\n",
       " 'LazyConvTranspose2d',\n",
       " 'LazyConvTranspose3d',\n",
       " 'LazyBatchNorm1d',\n",
       " 'LazyBatchNorm2d',\n",
       " 'LazyBatchNorm3d',\n",
       " 'LazyInstanceNorm1d',\n",
       " 'LazyInstanceNorm2d',\n",
       " 'LazyInstanceNorm3d',\n",
       " 'Flatten',\n",
       " 'Unflatten',\n",
       " 'Hardsigmoid',\n",
       " 'Hardswish',\n",
       " 'SiLU',\n",
       " 'Mish',\n",
       " 'TripletMarginWithDistanceLoss',\n",
       " 'ChannelShuffle',\n",
       " 'Parameter',\n",
       " 'UninitializedParameter',\n",
       " 'UninitializedBuffer',\n",
       " 'parallel',\n",
       " 'DataParallel',\n",
       " 'factory_kwargs',\n",
       " 'intrinsic',\n",
       " 'qat',\n",
       " 'quantized',\n",
       " 'quantizable']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.__dir__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       "[torch.storage._TypedStorage(dtype=torch.float32, device=cpu) of size 60]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.storage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin.in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet152()\n",
    "fe = feature_extraction.create_feature_extractor(model, ['avgpool'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.zeros((4, 3, 244, 244), device='meta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to('meta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "convolution_overrideable not implemented. You are likely triggering this with tensor backend other than CPU/CUDA/MKLDNN, if this is intended, please use TORCH_LIBRARY_IMPL to override this function ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [32], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model(x)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torchvision/models/resnet.py:285\u001b[0m, in \u001b[0;36mResNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 285\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_forward_impl(x)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torchvision/models/resnet.py:268\u001b[0m, in \u001b[0;36mResNet._forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_forward_impl\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m    267\u001b[0m     \u001b[39m# See note [TorchScript super()]\u001b[39;00m\n\u001b[0;32m--> 268\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv1(x)\n\u001b[1;32m    269\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn1(x)\n\u001b[1;32m    270\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(x)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/conv.py:457\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 457\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/conv.py:453\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    450\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    451\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    452\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 453\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    454\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: convolution_overrideable not implemented. You are likely triggering this with tensor backend other than CPU/CUDA/MKLDNN, if this is intended, please use TORCH_LIBRARY_IMPL to override this function "
     ]
    }
   ],
   "source": [
    "model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin = torch.nn.Linear(3, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 3])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin._parameters['weight'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "out= fe(x)['avgpool']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 2048, 1, 1])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MeanBackward1 at 0x7fc0613eef10>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.grad_fn = lambda x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.Tensor([\n",
    "    [\n",
    "        [k for k in range(3)] \n",
    "        for i in range(3)\n",
    "    ] for j in range(4)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 3])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 3])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin._parameters['weight'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 8])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(3, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.]]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones(1, requires_grad=True) * torch.ones((3, 4, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "new() received an invalid combination of arguments - got (list, requires_grad=bool), but expected one of:\n * (*, torch.device device)\n      didn't match because some of the keywords were incorrect: requires_grad\n * (torch.Storage storage)\n * (Tensor other)\n * (tuple of ints size, *, torch.device device)\n * (object data, *, torch.device device)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [96], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m torch\u001b[38;5;241m.\u001b[39mones((\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m)) \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor([\u001b[38;5;241m1\u001b[39m], requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mTypeError\u001b[0m: new() received an invalid combination of arguments - got (list, requires_grad=bool), but expected one of:\n * (*, torch.device device)\n      didn't match because some of the keywords were incorrect: requires_grad\n * (torch.Storage storage)\n * (Tensor other)\n * (tuple of ints size, *, torch.device device)\n * (object data, *, torch.device device)\n"
     ]
    }
   ],
   "source": [
    "torch.Tensor([1], requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = (1,2,3,4)\n",
    "kkt2 = (3,4,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2, 3, 5)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(*t1[:-1], t2[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.2000, 3.0000, 5.0000])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.2000, 1.2000, 1.2000, 1.2000, 3.0000, 3.0000, 3.0000, 3.0000, 5.0000,\n",
       "        5.0000, 5.0000, 5.0000])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.repeat_interleave(4, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "view() received an invalid combination of arguments - got (), but expected one of:\n * (tuple of ints size)\n * (torch.dtype dtype)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [64], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m torch\u001b[38;5;241m.\u001b[39mTensor\u001b[38;5;241m.\u001b[39mview(x)\n",
      "\u001b[0;31mTypeError\u001b[0m: view() received an invalid combination of arguments - got (), but expected one of:\n * (tuple of ints size)\n * (torch.dtype dtype)\n"
     ]
    }
   ],
   "source": [
    "torch.Tensor.view(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "out._requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (8) must match the existing size (0) at non-singleton dimension 3.  Target sizes: [4, 2048, 8, 8].  Tensor sizes: [0]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m out\u001b[38;5;241m.\u001b[39mgrad_fn(torch\u001b[38;5;241m.\u001b[39mTensor(\u001b[38;5;241m0\u001b[39m))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The expanded size of the tensor (8) must match the existing size (0) at non-singleton dimension 3.  Target sizes: [4, 2048, 8, 8].  Tensor sizes: [0]"
     ]
    }
   ],
   "source": [
    "out.grad_fn(torch.Tensor(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'out' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m out\u001b[38;5;241m.\u001b[39mgrad_fn\n",
      "\u001b[0;31mNameError\u001b[0m: name 'out' is not defined"
     ]
    }
   ],
   "source": [
    "out.grad_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['conv1', 'bn1', 'relu', 'maxpool', 'layer1', 'layer2', 'layer3', 'layer4', 'avgpool', 'fc'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model._modules.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mType:\u001b[0m        module\n",
      "\u001b[0;31mString form:\u001b[0m <module 'torchvision.models.feature_extraction' from '/home/nathan/.local/lib/python3.9/site-packages/torchvision/models/feature_extraction.py'>\n",
      "\u001b[0;31mFile:\u001b[0m        ~/.local/lib/python3.9/site-packages/torchvision/models/feature_extraction.py\n",
      "\u001b[0;31mDocstring:\u001b[0m   <no docstring>\n"
     ]
    }
   ],
   "source": [
    "feature_extraction.create_feature_extractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m\n",
      "\u001b[0mresnet152\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mweights\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResNet152_Weights\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mprogress\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResNet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "ResNet-152 from `Deep Residual Learning for Image Recognition <https://arxiv.org/pdf/1512.03385.pdf>`__.\n",
      "\n",
      ".. note::\n",
      "   The bottleneck of TorchVision places the stride for downsampling to the second 3x3\n",
      "   convolution while the original paper places it to the first 1x1 convolution.\n",
      "   This variant improves the accuracy and is known as `ResNet V1.5\n",
      "   <https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch>`_.\n",
      "\n",
      "Args:\n",
      "    weights (:class:`~torchvision.models.ResNet152_Weights`, optional): The\n",
      "        pretrained weights to use. See\n",
      "        :class:`~torchvision.models.ResNet152_Weights` below for\n",
      "        more details, and possible values. By default, no pre-trained\n",
      "        weights are used.\n",
      "    progress (bool, optional): If True, displays a progress bar of the\n",
      "        download to stderr. Default is True.\n",
      "    **kwargs: parameters passed to the ``torchvision.models.resnet.ResNet``\n",
      "        base class. Please refer to the `source code\n",
      "        <https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py>`_\n",
      "        for more details about this class.\n",
      "\n",
      ".. autoclass:: torchvision.models.ResNet152_Weights\n",
      "    :members:\n",
      "\u001b[0;31mFile:\u001b[0m      ~/.local/lib/python3.9/site-packages/torchvision/models/resnet.py\n",
      "\u001b[0;31mType:\u001b[0m      function\n"
     ]
    }
   ],
   "source": [
    "feature_extraction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('torchmocks')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f2e340c3c7003f18c0bf307d48499b3f088d05ccc6152e7379f50529a4df4f36"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
