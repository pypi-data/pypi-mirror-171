"""Helper module"""

__author__ = "Dilawar Singh"
__email__ = "dilawar@subcom.tech"

import hashlib
import sys
import uuid
import zipfile
import shutil
from pathlib import Path
from datetime import datetime
import tempfile
import typing as T

import validators
import requests

import bitia.config as bconfig
import bitia.session as bsession
from bitia.checksumdir import dirhash
from bitia.logger import logger


def log_container(container: str, server: str):
    assert (
        container
    ), "Failed to determine the container that is runnning the pipeline. There is probably a bug in server end."
    for line in bsession.fetch_logs(container, server=server):
        print(line.decode().rstrip())


def _check_server_status(server: str) -> int:
    res = requests.get(server)
    return res.status_code


def dir_info(user_dir: Path) -> dict:
    """Check if directory is in good condition."""
    files = [f.resolve() for f in user_dir.glob("**/*") if f.is_file()]
    size_in_mb = sum(f.stat().st_size / 1024.0 / 1024.0 for f in files)
    if size_in_mb > 25.0:
        logger.warning(
            "The size of pipeline is >25MB ({size_in_mb} MB)."
            " You should try to reduce the size of the pipeline. TODO: See this link."
        )
    return dict(size_in_mb=size_in_mb, num_files=len(files), files=files)


def prepare_archive(user_dir: Path) -> Path:
    """Prepare the file to upload. Store it in temp directory"""
    dinfo = dir_info(user_dir)
    dhash = dirhash(user_dir)
    outfile = bconfig.bitia_dir() / "pipelines" / f"{dhash}.zip"
    if outfile.is_file():
        logger.info(f"Reusing the existing pipeline `{outfile}`.")
        return outfile
    logger.info(f"Preparing the zipfile pipeline from {user_dir}")
    logger.info(f" size={dinfo['size_in_mb']} MB, total files={dinfo['num_files']}")
    outfile.parent.mkdir(parents=True, exist_ok=True)
    assert dinfo["files"], f"No file found in {user_dir}"
    with zipfile.ZipFile(outfile, "w", zipfile.ZIP_DEFLATED) as zfile:
        for entry in dinfo["files"]:
            logger.info(f"Adding {entry} to zipfile")
            zfile.write(entry)

    # check the prepared zip file.
    with zipfile.ZipFile(outfile) as zfile:
        assert zfile.namelist(), "Empty zipfile"

    # assert non-zero size of the zip file.
    assert outfile.is_file(), f"{outfile} does not exists"
    return outfile


def create_pipeline_from_single_script(script: Path) -> Path:
    """Create a pipelinefile from a single script"""
    assert script.is_file(), f"{script} is not a file"
    pipeline_dir = Path(tempfile.mkdtemp(prefix="bitia_"))
    pipeline_file = pipeline_dir / bconfig.BITIA_MAIN_SCRIPT_NAME
    # move the script to this directory.
    shutil.copy2(script, pipeline_dir)
    script_name = script.name
    with pipeline_file.open("w", newline="\n") as outf:
        outf.write(f"#!/bin/sh\nchmod +x ./{script_name}\n./{script_name}")
    return prepare_archive(pipeline_dir)


def create_pipeline_from_command(
    cmd: str, add_timestamp: bool = True, add_uuid1: bool = True
) -> Path:
    """Create a pipeline from user input.

    Returns
    -------
    The directory in which pipeline was created.
    """
    pipeline_dir = Path(tempfile.mkdtemp(prefix="bitia_"))
    pipeline_file = pipeline_dir / bconfig.BITIA_MAIN_SCRIPT_NAME
    with pipeline_file.open("w", newline="\n") as outf:
        outf.writeline("#!/bin/sh")
        if add_timestamp:
            outf.writeline(f"# timestamp={datetime.now().isoformat()}")
        if add_uuid1:
            outf.writeline(f"# uuid={uuid.uuid1()}")
        outf.writeline(f"{cmd}")
    logger.info("Wrote pipeline %s", pipeline_file.read_text())
    return prepare_archive(pipeline_dir)


def post_pipeline_task(
    pipeline_zip: Path,
    *,
    endpoint: str,
    server: str,
    params: T.Dict[str, str] = {},
    **kwargs,
):
    """Submit to the api for a given endpoint and pipeline file"""
    numbytes = pipeline_zip.stat().st_size
    if (code := _check_server_status(server)) != 200:
        logger.warning(
            "%s may not be alive (status code: %s). Try other one?", server, code
        )
        sys.exit(1)
    assert numbytes > 0
    logger.info(
        "Submitting %s (size=%.2f KB) to the %s",
        pipeline_zip,
        numbytes / 1024.0,
        server,
    )

    #  submit and print the output.
    endpoint = endpoint.strip("/")
    with pipeline_zip.open("rb") as f_pipeline:
        files = {"pipeline_zip": f_pipeline}
        return bsession.post(
            f"{server}/{endpoint}",
            files=files,
            params=params,
            data=dict(filename=pipeline_zip).update(**kwargs),
            stream=kwargs.get("stream", False),
        )


def post(
    endpoint: str,
    *,
    server: str,
    stream: bool = False,
    params: T.Dict[str, str] = {},
    **kwargs,
):
    """A generic post function."""
    logger.info(f"Posting with data {kwargs}")
    return bsession.post(
        f"{server}/{endpoint}", json=kwargs, params=params, stream=stream
    )


def get(
    endpoint: str,
    *,
    server: str,
    stream: bool = False,
    params: T.Dict[str, str] = {},
    **kwargs,
):
    """A generic post function."""
    logger.info(f"Posting with data {kwargs}")
    return bsession.get(
        f"{server}/{endpoint}", params=params, json=kwargs, stream=stream
    )


def submit_job(
    pipeline_zip: Path, *, server: str, rerun: bool = False, params: dict = {}
):
    """Submit job to the API and stream the output."""
    numbytes = pipeline_zip.stat().st_size
    if (code := _check_server_status(server)) != 200:
        logger.warning(
            "%s may not be alive (status code: %s). Try other one?", server, code
        )
        sys.exit(1)
    assert numbytes > 0
    logger.info(
        "Submitting %s (size=%.2f KB) to the %s",
        pipeline_zip,
        numbytes / 1024.0,
        server,
    )

    #  submit and print the output.
    with pipeline_zip.open("rb") as f_pipeline:
        files = {"pipeline_zip": f_pipeline}
        response = bsession.post(
            f"{server}/submit/?rerun={rerun}",
            files=files,
            params=params,
            json=dict(filename=pipeline_zip, rerun=rerun),
        )
        return response.json()


def sha256sum(infile: Path) -> str:
    """Compute sha256sum of a file.

    Credit
    ------
    Thanks https://stackoverflow.com/a/44873382/1805129

    """
    h = hashlib.sha256()
    b = bytearray(128 * 1024)
    mv = memoryview(b)
    with infile.open("rb", buffering=0) as handler:
        while n := handler.readinto(mv):
            h.update(mv[:n])
    return h.hexdigest()


def user_input_to_pipeline(user_input: str) -> T.Tuple[Path, str]:
    """Create a pipeline file from user_input"""
    if (path := Path(user_input)).exists():
        if path.is_dir():
            pipeline_zip = prepare_archive(path)
        elif path.is_file() and path.suffix.lower() == ".zip":
            pipeline_zip = path
        elif path.is_file():
            pipeline_zip = create_pipeline_from_single_script(path)
        else:
            raise NotImplementedError(f"{path} is not yet supported")
    elif validators.url(user_input):
        logger.warning("Fetching pipeline from url is not supported")
        sys.exit(-1)
    else:
        # generate a temporary pipeline and submit.
        pipeline_zip = create_pipeline_from_command(user_input)
        logger.info(f"Created pipeline in {pipeline_zip}")
    return pipeline_zip, sha256sum(pipeline_zip)
